{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bW444oomCxg",
        "outputId": "aea591a4-8ed7-4cd5-8a10-626e4fa732ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Preprocessed data successfully loaded!\n",
            "Training set size: 8000, Test set size: 2717\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install dependencies (only needed once)\n",
        "!pip install transformers datasets torch scikit-learn matplotlib tqdm nltk optuna\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 3: Choose data format (CSV or Parquet)\n",
        "DATA_FORMAT = \"csv\"  # Choose between \"csv\" or \"parquet\"\n",
        "\n",
        "# Step 4: Load the preprocessed dataset\n",
        "if DATA_FORMAT == \"csv\":\n",
        "    df_train = pd.read_csv(\"train_enhanced.csv\")\n",
        "    df_test = pd.read_csv(\"test_enhanced.csv\")\n",
        "elif DATA_FORMAT == \"parquet\":\n",
        "    df_train = pd.read_parquet(\"train_enhanced.parquet\", engine=\"pyarrow\")\n",
        "    df_test = pd.read_parquet(\"test_enhanced.parquet\", engine=\"pyarrow\")\n",
        "else:\n",
        "    raise ValueError(\"Invalid DATA_FORMAT. Please set to either 'csv' or 'parquet'.\")\n",
        "\n",
        "print(\"Preprocessed data successfully loaded!\")\n",
        "print(f\"Training set size: {len(df_train)}, Test set size: {len(df_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH8mNErpmIoJ",
        "outputId": "58eb1157-e1e8-4e7a-eb01-dbe37dab9d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Generate relation label mappings\n",
        "unique_relations = sorted(df_train[\"relation\"].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(unique_relations)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "df_train[\"label_id\"] = df_train[\"relation\"].map(label2id)\n",
        "df_test[\"label_id\"] = df_test[\"relation\"].map(label2id)\n",
        "\n",
        "# Step 6: Ensure GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 7: Load pre-trained BERT model and tokenizer\n",
        "num_labels = len(label2id)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Step 8: Tokenization\n",
        "def encode_texts(texts, tokenizer, max_length=256):\n",
        "    \"\"\"Tokenizes and encodes sentences into numerical format for BERT processing.\"\"\"\n",
        "    return tokenizer(list(texts.values), padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "# Convert text data into BERT-compatible format\n",
        "train_encodings = encode_texts(df_train[\"enhanced_sentence\"], tokenizer)\n",
        "test_encodings = encode_texts(df_test[\"enhanced_sentence\"], tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ybrw3JvmXy3",
        "outputId": "c252a05e-cbc0-4a21-e3f0-0d6233374d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data processing completed, DataLoader is ready!\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Create PyTorch Dataset class\n",
        "class RelationDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset class for relation extraction.\"\"\"\n",
        "\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Convert training and test data into PyTorch datasets\n",
        "train_dataset = RelationDataset(train_encodings, df_train[\"label_id\"].tolist())\n",
        "test_dataset = RelationDataset(test_encodings, df_test[\"label_id\"].tolist())\n",
        "\n",
        "# Step 10: Create DataLoader for batching\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\" Data processing completed, DataLoader is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mJGJtce-mc7n"
      },
      "outputs": [],
      "source": [
        "# Step 11: Define contrastive learning loss function\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"Custom contrastive loss function to improve relation classification by separating different relation types.\"\"\"\n",
        "\n",
        "    def __init__(self, temperature=0.1):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        \"\"\"Computes contrastive loss (pulls positive samples closer, pushes negative samples apart).\"\"\"\n",
        "        sim = torch.nn.functional.cosine_similarity(z_i, z_j, dim=-1) / self.temperature\n",
        "        loss = -torch.log(torch.nn.functional.softmax(sim, dim=-1)).mean()\n",
        "        return loss\n",
        "\n",
        "contrastive_loss_fn = ContrastiveLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bg2Koelmh56",
        "outputId": "18982499-616b-4284-cc76-37acfd818c76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Cross-Entropy Loss = 0.9780, Contrastive Loss = 3.7436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Cross-Entropy Loss = 0.8695, Contrastive Loss = 3.6688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:42<00:00,  5.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Cross-Entropy Loss = 0.8118, Contrastive Loss = 3.6310\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Cross-Entropy Loss = 0.7812, Contrastive Loss = 3.6037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Cross-Entropy Loss = 0.7611, Contrastive Loss = 3.5880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Cross-Entropy Loss = 0.7479, Contrastive Loss = 3.5705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Cross-Entropy Loss = 0.7368, Contrastive Loss = 3.5636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Cross-Entropy Loss = 0.7315, Contrastive Loss = 3.5534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Cross-Entropy Loss = 0.7285, Contrastive Loss = 3.5489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:43<00:00,  5.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Cross-Entropy Loss = 0.7273, Contrastive Loss = 3.5476\n",
            "Training completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 12: Train the model (with learning rate scheduling & contrastive learning)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Set up learning rate scheduler\n",
        "num_training_steps = len(train_loader) * 10  # 10 epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss, contrastive_loss_total = 0, 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through BERT\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        logits = outputs.logits\n",
        "        hidden_states = outputs.hidden_states[-1][:, 0, :]  # Extract CLS token representation\n",
        "\n",
        "        # Compute standard classification loss\n",
        "        ce_loss = criterion(logits, labels)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        positive_idx = torch.arange(hidden_states.size(0))\n",
        "        negative_idx = torch.roll(positive_idx, shifts=1)\n",
        "        contrastive_loss = contrastive_loss_fn(hidden_states[positive_idx], hidden_states[negative_idx])\n",
        "\n",
        "        # Total loss = cross-entropy loss + weighted contrastive loss\n",
        "        loss = ce_loss + 0.2 * contrastive_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        contrastive_loss_total += contrastive_loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Cross-Entropy Loss = {total_loss / len(train_loader):.4f}, Contrastive Loss = {contrastive_loss_total / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpqlxg0AmnQR",
        "outputId": "5820c5b7-ec53-4f1f-c159-420a8a5e18ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Evaluation on Test Set:\n",
            "Accuracy: 0.8524\n",
            "Precision: 0.8023\n",
            "Recall: 0.8205\n",
            "F1 Score: 0.8101\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93       134\n",
            "           1       0.92      0.92      0.92       194\n",
            "           2       0.87      0.87      0.87       162\n",
            "           3       0.79      0.81      0.80       150\n",
            "           4       0.90      0.93      0.92       153\n",
            "           5       0.92      0.90      0.91        39\n",
            "           6       0.92      0.95      0.94       291\n",
            "           7       1.00      0.00      0.00         1\n",
            "           8       0.85      0.92      0.89       211\n",
            "           9       0.93      0.87      0.90        47\n",
            "          10       0.63      0.77      0.69        22\n",
            "          11       0.88      0.79      0.83       134\n",
            "          12       0.69      0.75      0.72        32\n",
            "          13       0.91      0.89      0.90       201\n",
            "          14       0.86      0.95      0.90       210\n",
            "          15       0.83      0.94      0.88        51\n",
            "          16       0.87      0.84      0.85       108\n",
            "          17       0.85      0.91      0.88       123\n",
            "          18       0.72      0.62      0.67       454\n",
            "\n",
            "    accuracy                           0.85      2717\n",
            "   macro avg       0.85      0.82      0.81      2717\n",
            "weighted avg       0.85      0.85      0.85      2717\n",
            "\n",
            "Code execution completed, training and evaluation finished!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Step 13: Model evaluation\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='macro')  # Compute Precision\n",
        "recall = recall_score(all_labels, all_preds, average='macro')        # Compute Recall\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')                # Compute F1-score\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Model Evaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[str(label) for label in label2id.keys()], zero_division=1))\n",
        "\n",
        "print(\"Code execution completed, training and evaluation finished!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNuIjMS0oBfk",
        "outputId": "51a491ae-f513-4ef3-99d0-d2dd0bcc94b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model weights and configuration...\n",
            "Saving tokenizer...\n",
            " Model, tokenizer, and training state successfully saved in 'improved_bert_relation_model'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "#  Define the directory to save the model\n",
        "model_path = \"improved_bert_relation_model\"\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "#  Save the trained model (this saves 'pytorch_model.bin' and 'config.json')\n",
        "print(\"Saving model weights and configuration...\")\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "#  Save the tokenizer (this saves 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt')\n",
        "print(\"Saving tokenizer...\")\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "# Save label mappings (ensuring integer keys and values are converted to standard Python integers)\n",
        "label_mappings = {\"label2id\": {int(k): int(v) for k, v in label2id.items()},\n",
        "                  \"id2label\": {int(k): str(v) for k, v in id2label.items()}} # Convert values to strings\n",
        "with open(os.path.join(model_path, \"label_mappings.json\"), \"w\") as f:\n",
        "    json.dump(label_mappings, f)\n",
        "\n",
        "# Save optimizer and scheduler states (optional, useful for resuming training)\n",
        "torch.save({\n",
        "    'epoch': 10,  # Save last completed epoch\n",
        "    'model_state_dict': model.state_dict(),  # Save model weights\n",
        "    'optimizer_state_dict': optimizer.state_dict(),  # Save optimizer state\n",
        "    'scheduler_state_dict': lr_scheduler.state_dict(),  # Save learning rate scheduler state\n",
        "}, os.path.join(model_path, \"checkpoint.pth\"))\n",
        "\n",
        "print(f\" Model, tokenizer, and training state successfully saved in '{model_path}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIN-12GqoSNu",
        "outputId": "834136e7-ae26-4d2c-d7a8-929bfd92d1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer successfully loaded from: improved_bert_relation_model\n"
          ]
        }
      ],
      "source": [
        "# Step 15: Load the saved model, tokenizer, and label mappings\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Define model path\n",
        "model_path = \"improved_bert_relation_model\"\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load label mappings\n",
        "with open(os.path.join(model_path, \"label_mappings.json\"), \"r\") as f:\n",
        "    label_mappings = json.load(f)\n",
        "    label2id = label_mappings[\"label2id\"]\n",
        "    id2label = label_mappings[\"id2label\"]\n",
        "\n",
        "# Move model to the available device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model and tokenizer successfully loaded from: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ8IxujgsNmy",
        "outputId": "9909b9b8-a252-47aa-858d-bd8091c9515b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Relation: 18\n"
          ]
        }
      ],
      "source": [
        "def predict_relation(sentence, model, tokenizer, id2label):\n",
        "    \"\"\"\n",
        "    Predicts the relation type for a given input sentence using the trained BERT model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): A sentence with entity markers <e1>...</e1> and <e2>...</e2>.\n",
        "        model (BertForSequenceClassification): The trained BERT model.\n",
        "        tokenizer (BertTokenizer): Tokenizer corresponding to the trained BERT model.\n",
        "        id2label (dict): Dictionary mapping label IDs to relation names.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted relation label.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input sentence for BERT processing\n",
        "    inputs = tokenizer(sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations for inference\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # Retrieve the predicted relation label\n",
        "    relation = id2label[str(prediction)]\n",
        "    return relation\n",
        "\n",
        "# Example usage\n",
        "test_sentence = \"The <e1>company</e1> acquired the <e2>startup</e2>.\"\n",
        "predicted_relation = predict_relation(test_sentence, model, tokenizer, id2label)\n",
        "\n",
        "print(f\"Predicted Relation: {predicted_relation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awE-FCaetJw1",
        "outputId": "afa993bf-b972-479c-a2b2-5ae12bec5ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction without external knowledge: 18\n",
            "Prediction with external knowledge: 18\n"
          ]
        }
      ],
      "source": [
        "# Example with and without external knowledge\n",
        "test_sentence_basic = \"The <e1>scientist</e1> won the <e2>Nobel Prize</e2>.\"\n",
        "test_sentence_augmented = (\n",
        "    \"The <e1>scientist</e1> won the <e2>Nobel Prize</e2>. \"\n",
        "    \"[SEP] scientist: A person who is studying or has expert knowledge in science. \"\n",
        "    \"[KG: works at: Research Institute] [SEP] Nobel Prize: An international award given for achievements.\"\n",
        ")\n",
        "\n",
        "# Predict relations\n",
        "relation_basic = predict_relation(test_sentence_basic, model, tokenizer, id2label)\n",
        "relation_augmented = predict_relation(test_sentence_augmented, model, tokenizer, id2label)\n",
        "\n",
        "print(f\"Prediction without external knowledge: {relation_basic}\")\n",
        "print(f\"Prediction with external knowledge: {relation_augmented}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_w99BOJ-7ETj",
        "outputId": "66965e9b-65d2-4510-901e-e051ef73f573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Loading model and tokenizer...\n",
            " Model successfully loaded onto: cuda\n",
            "\n",
            "Relation Extraction Model Ready! Type 'exit' to stop.\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\n",
            "Predicted Relation ID: 14, Predicted Relation Name: Message-Topic(e1,e2)\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "The <e1>company</e1> fabricates plastic <e2>chairs</e2>.\n",
            "Predicted Relation ID: 17, Predicted Relation Name: Product-Producer(e2,e1)\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "The school <e1>master</e1> teaches the lesson with a <e2>stick</e2>.\n",
            "Predicted Relation ID: 11, Predicted Relation Name: Instrument-Agency(e2,e1)\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "The suspect dumped the dead <e1>body</e1> into a local <e2>reservoir</e2>.\n",
            "Predicted Relation ID: 6, Predicted Relation Name: Entity-Destination(e1,e2)\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "Avian <e1>influenza</e1> is an infectious disease of birds caused by type A strains of the influenza <e2>virus</e2>.\n",
            "Predicted Relation ID: 1, Predicted Relation Name: Cause-Effect(e2,e1)\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "The <e1>ear</e1> of the African <e2>elephant</e2> is significantly larger--measuring 183 cm by 114 cm in the bush elephant.\n",
            "Predicted Relation ID: 2, Predicted Relation Name: Component-Whole(e1,e2)\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "A child is told a <e1>lie</e1> for several years by their <e2>parents</e2> before he/she realizes that a Santa Claus does not exist.\n",
            "Predicted Relation ID: 18, Predicted Relation Name: Other\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "Skype, a free software, allows a <e1>hookup</e1> of multiple computer <e2>users</e2> to join in an online conference call without incurring any telephone costs.\n",
            "Predicted Relation ID: 18, Predicted Relation Name: Other\n",
            "\n",
            "Enter a sentence with <e1> and <e2> entity markers:\n",
            "This <e1>thesis</e1> defines the <e2>clinical characteristics</e2> of amyloid disease.\n",
            "Predicted Relation ID: 14, Predicted Relation Name: Message-Topic(e1,e2)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a28d63be746d>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0muser_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter a sentence with <e1> and <e2> entity markers:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Step 1: Define the directory containing the saved model\n",
        "model_path = \"/content/improved_bert_relation_model\"\n",
        "\n",
        "# Step 2: Load the trained model and tokenizer\n",
        "print(\" Loading model and tokenizer...\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Step 3: Load label mappings\n",
        "with open(os.path.join(model_path, \"label_mappings.json\"), \"r\") as f:\n",
        "    label_mappings = json.load(f)\n",
        "    label2id = label_mappings[\"label2id\"]\n",
        "    id2label = {int(k): v for k, v in label_mappings[\"id2label\"].items()}  # Ensure keys are integers\n",
        "\n",
        "# Step 4: Set device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\" Model successfully loaded onto: {device}\")\n",
        "\n",
        "# Step 5: Define function for relation extraction\n",
        "def predict_relation(sentence, model, tokenizer, id2label):\n",
        "    \"\"\"\n",
        "    Predicts the relation type for a given input sentence using the trained BERT model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): A sentence with entity markers <e1>...</e1> and <e2>...</e2>.\n",
        "        model (BertForSequenceClassification): The trained BERT model.\n",
        "        tokenizer (BertTokenizer): Tokenizer corresponding to the trained BERT model.\n",
        "        id2label (dict): Dictionary mapping label IDs to relation names.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The predicted relation ID and relation label.\n",
        "    \"\"\"\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Check if the sentence contains entity markers\n",
        "    if \"<e1>\" not in sentence or \"<e2>\" not in sentence:\n",
        "        return \"Error: The input sentence must contain <e1>...</e1> and <e2>...</e2> markers.\"\n",
        "\n",
        "    # Tokenize input sentence for BERT processing\n",
        "    inputs = tokenizer(sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations for inference\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # Retrieve the predicted relation label\n",
        "    relation_id = prediction\n",
        "    relation_name = id2label[prediction]\n",
        "    return relation_id, relation_name\n",
        "\n",
        "# Step 6: Real-time inference loop (User input mode)\n",
        "print(\"\\nRelation Extraction Model Ready! Type 'exit' to stop.\")\n",
        "\n",
        "# Define the relation_id_to_label mapping\n",
        "relation_id_to_label = {\n",
        "    0: \"Cause-Effect(e1,e2)\",\n",
        "    1: \"Cause-Effect(e2,e1)\",\n",
        "    2: \"Component-Whole(e1,e2)\",\n",
        "    3: \"Component-Whole(e2,e1)\",\n",
        "    4: \"Content-Container(e1,e2)\",\n",
        "    5: \"Content-Container(e2,e1)\",\n",
        "    6: \"Entity-Destination(e1,e2)\",\n",
        "    7: \"Entity-Destination(e2,e1)\",\n",
        "    8: \"Entity-Origin(e1,e2)\",\n",
        "    9: \"Entity-Origin(e2,e1)\",\n",
        "    10: \"Instrument-Agency(e1,e2)\",\n",
        "    11: \"Instrument-Agency(e2,e1)\",\n",
        "    12: \"Member-Collection(e1,e2)\",\n",
        "    13: \"Member-Collection(e2,e1)\",\n",
        "    14: \"Message-Topic(e1,e2)\",\n",
        "    15: \"Message-Topic(e2,e1)\",\n",
        "    16: \"Product-Producer(e1,e2)\",\n",
        "    17: \"Product-Producer(e2,e1)\",\n",
        "    18: \"Other\"\n",
        "}\n",
        "\n",
        "while True:\n",
        "    user_sentence = input(\"\\nEnter a sentence with <e1> and <e2> entity markers:\\n\")\n",
        "\n",
        "    if user_sentence.lower() == \"exit\":\n",
        "        print(\"Exiting program...\")\n",
        "        break\n",
        "\n",
        "    relation_id, relation_name = predict_relation(user_sentence, model, tokenizer, relation_id_to_label)\n",
        "    print(f\"Predicted Relation ID: {relation_id}, Predicted Relation Name: {relation_name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
