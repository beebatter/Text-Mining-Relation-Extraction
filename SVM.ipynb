{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOX0wWpaJDo6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **First testing for reading dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTglkDHZ3rpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  set path of document\n",
        "file_path = '/content/train-00000-of-00001.parquet'\n",
        "\n",
        "# using function read_parquet in pandas to read Parquert documents\n",
        "df = pd.read_parquet(file_path, engine='pyarrow')\n",
        "\n",
        "# show first few lines in the  dataset\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "QN4yEnDwfI38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda2df98-30a3-4956-ce92-e89da629c049"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence  relation\n",
            "0  The system as described above has its greatest...         3\n",
            "1  The <e1>child</e1> was carefully wrapped and b...        18\n",
            "2  The <e1>author</e1> of a keygen uses a <e2>dis...        11\n",
            "3  A misty <e1>ridge</e1> uprises from the <e2>su...        18\n",
            "4  The <e1>student</e1> <e2>association</e2> is t...        12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 1.initial version, using CountVectorizer for feature extraction (with poor performance)**"
      ],
      "metadata": {
        "id": "kk4V_YFk379_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score # Import precision_score, recall_score, f1_score\n",
        "\n",
        "# read training set\n",
        "train_file_path = '/content/train-00000-of-00001.parquet'\n",
        "train_df = pd.read_parquet(train_file_path, engine='pyarrow')\n",
        "\n",
        "# reading test set\n",
        "test_file_path = '/content/test-00000-of-00001.parquet'\n",
        "test_df = pd.read_parquet(test_file_path, engine='pyarrow')\n",
        "\n",
        "#  extracting features(sentences) and labels(relations)\n",
        "X_train = train_df['sentence']\n",
        "y_train = train_df['relation']\n",
        "\n",
        "X_test = test_df['sentence']\n",
        "y_test = test_df['relation']\n",
        "\n",
        "\n",
        "# creating pipepline, with using CountVectorizer to translate contexts into word frequncy, then using SVM to classify\n",
        "model = make_pipeline(CountVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# print the result of classification\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# print the scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(\"Model Evaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LRAwM3qQw7n",
        "outputId": "53f755d1-7f2f-44e2-85ec-ab7097452d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.67      0.76       134\n",
            "           1       0.75      0.82      0.79       194\n",
            "           2       0.41      0.47      0.44       162\n",
            "           3       0.40      0.29      0.33       150\n",
            "           4       0.62      0.71      0.66       153\n",
            "           5       0.71      0.64      0.68        39\n",
            "           6       0.70      0.81      0.75       291\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.60      0.72      0.65       211\n",
            "           9       0.73      0.23      0.35        47\n",
            "          10       0.38      0.27      0.32        22\n",
            "          11       0.51      0.40      0.45       134\n",
            "          12       0.46      0.19      0.27        32\n",
            "          13       0.49      0.56      0.52       201\n",
            "          14       0.57      0.47      0.51       210\n",
            "          15       0.56      0.37      0.45        51\n",
            "          16       0.41      0.34      0.37       108\n",
            "          17       0.42      0.23      0.30       123\n",
            "          18       0.29      0.37      0.33       454\n",
            "\n",
            "    accuracy                           0.53      2717\n",
            "   macro avg       0.52      0.45      0.47      2717\n",
            "weighted avg       0.53      0.53      0.52      2717\n",
            "\n",
            "Model Evaluation on Test Set:\n",
            "Accuracy: 0.5252\n",
            "Precision: 0.5207\n",
            "Recall: 0.4502\n",
            "F1 Score: 0.4694\n",
            "\n",
            "Detailed Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.using IF-IDF for Text Augmentation**"
      ],
      "metadata": {
        "id": "RZXJYM8C4Dje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "train_file_path = '/content/train-00000-of-00001.parquet'\n",
        "train_df = pd.read_parquet(train_file_path, engine='pyarrow')\n",
        "\n",
        "test_file_path = '/content/test-00000-of-00001.parquet'\n",
        "test_df = pd.read_parquet(test_file_path, engine='pyarrow')\n",
        "\n",
        "X_train = train_df['sentence']\n",
        "y_train = train_df['relation']\n",
        "\n",
        "X_test = test_df['sentence']\n",
        "y_test = test_df['relation']\n",
        "\n",
        "# creating pipepline, with using IF-IDF to vectorize texts, then using SVM to classify\n",
        "model = make_pipeline(TfidfVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# print the scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(\"Model Evaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAWK0neyq2q-",
        "outputId": "d190f004-fd00-4d70-ee68-7025eb349c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.68      0.79       134\n",
            "           1       0.82      0.81      0.81       194\n",
            "           2       0.49      0.51      0.50       162\n",
            "           3       0.59      0.25      0.36       150\n",
            "           4       0.69      0.69      0.69       153\n",
            "           5       0.70      0.59      0.64        39\n",
            "           6       0.74      0.82      0.78       291\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.61      0.76      0.68       211\n",
            "           9       0.90      0.19      0.32        47\n",
            "          10       0.50      0.14      0.21        22\n",
            "          11       0.60      0.37      0.46       134\n",
            "          12       0.33      0.03      0.06        32\n",
            "          13       0.56      0.56      0.56       201\n",
            "          14       0.66      0.42      0.51       210\n",
            "          15       0.74      0.33      0.46        51\n",
            "          16       0.49      0.35      0.41       108\n",
            "          17       0.42      0.11      0.17       123\n",
            "          18       0.27      0.51      0.36       454\n",
            "\n",
            "    accuracy                           0.54      2717\n",
            "   macro avg       0.58      0.43      0.46      2717\n",
            "weighted avg       0.59      0.54      0.53      2717\n",
            "\n",
            "Model Evaluation on Test Set:\n",
            "Accuracy: 0.5377\n",
            "Precision: 0.5815\n",
            "Recall: 0.4277\n",
            "F1 Score: 0.4606\n",
            "\n",
            "Detailed Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compared with countvectorizer, using IF-IDF\n",
        "has a little improvement in Accuracy (around 1%)"
      ],
      "metadata": {
        "id": "XqJSlDSRRnVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.including grid search: trying different kernal function parameters(C, gamma values); kernel functions**"
      ],
      "metadata": {
        "id": "XzN1S6AU4MUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "train_file_path = '/content/train-00000-of-00001.parquet'\n",
        "train_df = pd.read_parquet(train_file_path, engine='pyarrow')\n",
        "\n",
        "test_file_path = '/content/test-00000-of-00001.parquet'\n",
        "test_df = pd.read_parquet(test_file_path, engine='pyarrow')\n",
        "\n",
        "X_train = train_df['sentence']\n",
        "y_train = train_df['relation']\n",
        "\n",
        "X_test = test_df['sentence']\n",
        "y_test = test_df['relation']\n",
        "\n",
        "pipeline = make_pipeline(TfidfVectorizer(), SVC())\n",
        "\n",
        "# defining the parameters grid search used in SVM model\n",
        "param_grid = {\n",
        "    'svc__C': [0.1, 1, 10],  # SVM regularization parameter\n",
        "    'svc__gamma': [0.1, 1, 10],  # Kernel function parameter\n",
        "    'svc__kernel': ['linear', 'rbf', 'poly']  # Kernel function type\n",
        "}\n",
        "\n",
        "# creating the grid-search object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='f1_macro')\n",
        "\n",
        "# fit training data into the grid_serach object\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# output the optimal parameters and scores\n",
        "print(f'The best parameter is {grid_search.best_params_}')\n",
        "print(f'The best score is {grid_search.best_score_}')\n",
        "print(f'The best estimator is {grid_search.best_estimator_}')\n",
        "\n",
        "# predicting using the optimal model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# evaluating performance of model\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "# print the scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(\"Model Evaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQJpdtgjk2wy",
        "outputId": "45f2b27e-5905-4970-b525-9428b747c445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameter is {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel': 'linear'}\n",
            "The best score is 0.49765151341862734\n",
            "The best estimator is Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
            "                ('svc', SVC(C=10, gamma=0.1, kernel='linear'))])\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.72      0.80       134\n",
            "           1       0.79      0.84      0.81       194\n",
            "           2       0.41      0.54      0.47       162\n",
            "           3       0.41      0.31      0.35       150\n",
            "           4       0.67      0.70      0.68       153\n",
            "           5       0.66      0.59      0.62        39\n",
            "           6       0.74      0.82      0.78       291\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.63      0.71      0.67       211\n",
            "           9       0.79      0.32      0.45        47\n",
            "          10       0.43      0.27      0.33        22\n",
            "          11       0.55      0.40      0.46       134\n",
            "          12       0.62      0.16      0.25        32\n",
            "          13       0.57      0.56      0.56       201\n",
            "          14       0.59      0.50      0.54       210\n",
            "          15       0.66      0.41      0.51        51\n",
            "          16       0.43      0.37      0.40       108\n",
            "          17       0.48      0.21      0.29       123\n",
            "          18       0.28      0.39      0.33       454\n",
            "\n",
            "    accuracy                           0.54      2717\n",
            "   macro avg       0.56      0.46      0.49      2717\n",
            "weighted avg       0.56      0.54      0.54      2717\n",
            "\n",
            "Model Evaluation on Test Set:\n",
            "Accuracy: 0.5410\n",
            "Precision: 0.5579\n",
            "Recall: 0.4631\n",
            "F1 Score: 0.4895\n",
            "\n",
            "Detailed Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared with the initial SVM method,\n",
        "there are improvments in those scores\n",
        "from test sets by adding grid search to\n",
        "search best parameters. (improved appx 1% in Accuracy)"
      ],
      "metadata": {
        "id": "rf0xC8WP9vW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.using BiLSTM+Attention to extract features from texts,then input extracted features to SVM for relation extraction**"
      ],
      "metadata": {
        "id": "FuuZ2AyU4Rh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score # Import precision_score, recall_score, f1_score\n",
        "from joblib import load\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention, GlobalAveragePooling1D # Import GlobalAveragePooling1D\n",
        "\n",
        "train_file_path = '/content/train-00000-of-00001.parquet'\n",
        "train_df = pd.read_parquet(train_file_path, engine='pyarrow')\n",
        "\n",
        "test_file_path = '/content/test-00000-of-00001.parquet'\n",
        "test_df = pd.read_parquet(test_file_path, engine='pyarrow')\n",
        "\n",
        "X_train = train_df['sentence']\n",
        "y_train = train_df['relation']\n",
        "\n",
        "X_test = test_df['sentence']\n",
        "y_test = test_df['relation']\n",
        "\n",
        "\n",
        "# translate texts into sequences using Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# fill the sequences\n",
        "max_len = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "# defining BiLSTM + Attention model\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=100)(input_layer)\n",
        "bilstm_layer = Bidirectional(LSTM(units=64, return_sequences=True))(embedding_layer)\n",
        "attention_layer = tf.keras.layers.Attention()([bilstm_layer, bilstm_layer])\n",
        "\n",
        "# Add GlobalAveragePooling1D layer to get a fixed-length vector\n",
        "pooling_layer = GlobalAveragePooling1D()(attention_layer)\n",
        "output_layer = Dense(units=len(y_train.unique()), activation='softmax')(pooling_layer) # Connect to pooling layer\n",
        "\n",
        "# building model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# training deep learning model\n",
        "model.fit(X_train_pad, y_train, epochs=30, validation_split=0.2)\n",
        "\n",
        "# extracting features using the deep learning model\n",
        "train_features = model.predict(X_train_pad)\n",
        "test_features = model.predict(X_test_pad)\n",
        "\n",
        "# classification using SVM\n",
        "svm_model = make_pipeline(SVC(kernel='linear'))\n",
        "svm_model.fit(train_features, y_train)\n",
        "\n",
        "# predication and evaluation\n",
        "y_pred = svm_model.predict(test_features)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"Model Evaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VRFcJgAMk41",
        "outputId": "b523c65b-65fe-4663-81fe-d01d1cecf674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1464 - loss: 2.7398 - val_accuracy: 0.2394 - val_loss: 2.6894\n",
            "Epoch 2/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.1879 - loss: 2.5893 - val_accuracy: 0.2456 - val_loss: 2.4474\n",
            "Epoch 3/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.3566 - loss: 2.0723 - val_accuracy: 0.3137 - val_loss: 2.1892\n",
            "Epoch 4/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.5469 - loss: 1.4240 - val_accuracy: 0.3369 - val_loss: 2.1386\n",
            "Epoch 5/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.7118 - loss: 0.9616 - val_accuracy: 0.3981 - val_loss: 2.0761\n",
            "Epoch 6/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8199 - loss: 0.6329 - val_accuracy: 0.3925 - val_loss: 2.3312\n",
            "Epoch 7/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.8866 - loss: 0.4203 - val_accuracy: 0.4050 - val_loss: 2.4239\n",
            "Epoch 8/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9322 - loss: 0.2665 - val_accuracy: 0.3856 - val_loss: 2.6885\n",
            "Epoch 9/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.9517 - loss: 0.1944 - val_accuracy: 0.4144 - val_loss: 2.9118\n",
            "Epoch 10/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9709 - loss: 0.1345 - val_accuracy: 0.4094 - val_loss: 2.7301\n",
            "Epoch 11/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9816 - loss: 0.0873 - val_accuracy: 0.4069 - val_loss: 3.1124\n",
            "Epoch 12/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9833 - loss: 0.0731 - val_accuracy: 0.4112 - val_loss: 3.2654\n",
            "Epoch 13/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9851 - loss: 0.0727 - val_accuracy: 0.4181 - val_loss: 3.3007\n",
            "Epoch 14/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9908 - loss: 0.0504 - val_accuracy: 0.4325 - val_loss: 3.2456\n",
            "Epoch 15/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9919 - loss: 0.0427 - val_accuracy: 0.4106 - val_loss: 3.6518\n",
            "Epoch 16/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9931 - loss: 0.0380 - val_accuracy: 0.4212 - val_loss: 3.5623\n",
            "Epoch 17/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9943 - loss: 0.0294 - val_accuracy: 0.4281 - val_loss: 3.4602\n",
            "Epoch 18/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9963 - loss: 0.0215 - val_accuracy: 0.4006 - val_loss: 3.8237\n",
            "Epoch 19/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.9904 - loss: 0.0393 - val_accuracy: 0.4175 - val_loss: 3.8129\n",
            "Epoch 20/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9984 - loss: 0.0125 - val_accuracy: 0.3938 - val_loss: 3.6031\n",
            "Epoch 21/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9912 - loss: 0.0356 - val_accuracy: 0.4219 - val_loss: 3.5546\n",
            "Epoch 22/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9983 - loss: 0.0127 - val_accuracy: 0.4212 - val_loss: 3.7250\n",
            "Epoch 23/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9963 - loss: 0.0163 - val_accuracy: 0.3975 - val_loss: 4.0512\n",
            "Epoch 24/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9862 - loss: 0.0501 - val_accuracy: 0.4025 - val_loss: 3.6557\n",
            "Epoch 25/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9961 - loss: 0.0176 - val_accuracy: 0.4206 - val_loss: 3.8668\n",
            "Epoch 26/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9949 - loss: 0.0206 - val_accuracy: 0.4137 - val_loss: 3.7885\n",
            "Epoch 27/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0123 - val_accuracy: 0.4075 - val_loss: 4.1064\n",
            "Epoch 28/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9943 - loss: 0.0280 - val_accuracy: 0.4200 - val_loss: 3.7085\n",
            "Epoch 29/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9983 - loss: 0.0075 - val_accuracy: 0.4169 - val_loss: 3.9543\n",
            "Epoch 30/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9995 - loss: 0.0031 - val_accuracy: 0.4087 - val_loss: 4.1440\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "Model Evaluation on Test Set:\n",
            "Accuracy: 0.5679\n",
            "Precision: 0.5260\n",
            "Recall: 0.5064\n",
            "F1 Score: 0.5112\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80       134\n",
            "           1       0.79      0.82      0.81       194\n",
            "           2       0.53      0.52      0.53       162\n",
            "           3       0.46      0.41      0.43       150\n",
            "           4       0.71      0.72      0.72       153\n",
            "           5       0.72      0.59      0.65        39\n",
            "           6       0.80      0.78      0.79       291\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.71      0.71      0.71       211\n",
            "           9       0.54      0.60      0.57        47\n",
            "          10       0.20      0.23      0.21        22\n",
            "          11       0.46      0.42      0.44       134\n",
            "          12       0.15      0.34      0.21        32\n",
            "          13       0.68      0.75      0.71       201\n",
            "          14       0.66      0.44      0.53       210\n",
            "          15       0.51      0.41      0.46        51\n",
            "          16       0.48      0.42      0.45       108\n",
            "          17       0.39      0.32      0.35       123\n",
            "          18       0.32      0.39      0.35       454\n",
            "\n",
            "    accuracy                           0.57      2717\n",
            "   macro avg       0.53      0.51      0.51      2717\n",
            "weighted avg       0.59      0.57      0.57      2717\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the glove file\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifx1ZFzRBNqe",
        "outputId": "6b38c73c-20e1-42fb-f181-87bc9f1a44ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 01:28:22--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-02-27 01:28:22--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-02-27 01:28:23--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 3m 52s  \n",
            "\n",
            "2025-02-27 01:32:16 (3.55 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Glove word vectors in the pre-processing step has good\n",
        "improvement in Accuracy compared with the previous version (appx 10%)."
      ],
      "metadata": {
        "id": "3IkOvgO54pyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.adding pre-trained word vectors from GloVe to help the model understand meanings of words, with class weights**"
      ],
      "metadata": {
        "id": "N3rLOgth6IE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, GlobalAveragePooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score # Import precision_score, recall_score, f1_score\n",
        "from joblib import load\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention, GlobalAveragePooling1D # Import GlobalAveragePooling1D\n",
        "\n",
        "train_file_path = '/content/train-00000-of-00001.parquet'\n",
        "test_file_path = '/content/test-00000-of-00001.parquet'\n",
        "train_df = pd.read_parquet(train_file_path, engine='pyarrow')\n",
        "test_df = pd.read_parquet(test_file_path, engine='pyarrow')\n",
        "\n",
        "X_train = train_df['sentence'].tolist()\n",
        "y_train = train_df['relation'].tolist()\n",
        "X_test = test_df['sentence'].tolist()\n",
        "y_test = test_df['relation'].tolist()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "glove_path = \"/content/glove.6B.100d.txt\"\n",
        "embedding_dim = 100\n",
        "\n",
        "#adding wordnet\n",
        "word2vec = {}\n",
        "with open(glove_path, 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        word2vec[word] = vector\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    vector = word2vec.get(word)\n",
        "    if vector is not None:\n",
        "        embedding_matrix[i] = vector\n",
        "\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_len = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
        "bilstm_layer = Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2))(embedding_layer)\n",
        "attention_layer = tf.keras.layers.Attention()([bilstm_layer, bilstm_layer])\n",
        "pooling_layer = GlobalAveragePooling1D()(attention_layer)\n",
        "dropout_layer = Dropout(0.3)(pooling_layer)\n",
        "output_layer = Dense(units=len(np.unique(y_train_encoded)), activation='softmax')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "              loss=SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "model.fit(X_train_pad, y_train_encoded, epochs=30, batch_size=32, validation_split=0.2,\n",
        "           class_weight=class_weight_dict, callbacks=[early_stopping])\n",
        "\n",
        "train_features = model.predict(X_train_pad)\n",
        "test_features = model.predict(X_test_pad)\n",
        "\n",
        "svm_model = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_model.fit(train_features, y_train_encoded)\n",
        "\n",
        "\n",
        "y_pred = svm_model.predict(test_features)\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "precision = precision_score(y_test_encoded, y_pred, average='macro')\n",
        "recall = recall_score(y_test_encoded, y_pred, average='macro')\n",
        "f1 = f1_score(y_test_encoded, y_pred, average='macro')\n",
        "\n",
        "print(\"Model Evaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyAqZ2NypvTV",
        "outputId": "43a2d7cd-1a58-4f3f-9977-3fc2d8c89935"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.0901 - loss: 3.1371 - val_accuracy: 0.1281 - val_loss: 2.7200\n",
            "Epoch 2/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.1889 - loss: 3.5132 - val_accuracy: 0.1581 - val_loss: 2.6452\n",
            "Epoch 3/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.2688 - loss: 2.1732 - val_accuracy: 0.1631 - val_loss: 2.6875\n",
            "Epoch 4/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.3004 - loss: 2.1297 - val_accuracy: 0.2125 - val_loss: 2.4201\n",
            "Epoch 5/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.3424 - loss: 2.0166 - val_accuracy: 0.2625 - val_loss: 2.3448\n",
            "Epoch 6/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.3982 - loss: 1.7172 - val_accuracy: 0.2544 - val_loss: 2.3432\n",
            "Epoch 7/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.4461 - loss: 1.5837 - val_accuracy: 0.3356 - val_loss: 2.0986\n",
            "Epoch 8/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.4979 - loss: 1.4318 - val_accuracy: 0.3706 - val_loss: 1.9654\n",
            "Epoch 9/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.5469 - loss: 1.2921 - val_accuracy: 0.3850 - val_loss: 1.9831\n",
            "Epoch 10/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.5584 - loss: 1.2333 - val_accuracy: 0.4313 - val_loss: 1.7809\n",
            "Epoch 11/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.5985 - loss: 1.0764 - val_accuracy: 0.4494 - val_loss: 1.7359\n",
            "Epoch 12/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.6161 - loss: 1.0135 - val_accuracy: 0.4538 - val_loss: 1.7200\n",
            "Epoch 13/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.6505 - loss: 0.9314 - val_accuracy: 0.4869 - val_loss: 1.7052\n",
            "Epoch 14/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.6421 - loss: 0.8881 - val_accuracy: 0.4963 - val_loss: 1.6142\n",
            "Epoch 15/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.6665 - loss: 0.8258 - val_accuracy: 0.4950 - val_loss: 1.6028\n",
            "Epoch 16/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.6788 - loss: 0.7961 - val_accuracy: 0.4963 - val_loss: 1.6048\n",
            "Epoch 17/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.6877 - loss: 0.7274 - val_accuracy: 0.5188 - val_loss: 1.5633\n",
            "Epoch 18/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.6998 - loss: 0.7267 - val_accuracy: 0.5244 - val_loss: 1.4903\n",
            "Epoch 19/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.6910 - loss: 0.7174 - val_accuracy: 0.5300 - val_loss: 1.4795\n",
            "Epoch 20/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7186 - loss: 0.6426 - val_accuracy: 0.5425 - val_loss: 1.4461\n",
            "Epoch 21/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.7092 - loss: 0.6431 - val_accuracy: 0.5163 - val_loss: 1.5352\n",
            "Epoch 22/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.7179 - loss: 0.6333 - val_accuracy: 0.5444 - val_loss: 1.4445\n",
            "Epoch 23/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.7313 - loss: 0.5838 - val_accuracy: 0.5281 - val_loss: 1.4933\n",
            "Epoch 24/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7297 - loss: 0.5716 - val_accuracy: 0.5312 - val_loss: 1.4717\n",
            "Epoch 25/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7388 - loss: 0.5547 - val_accuracy: 0.5512 - val_loss: 1.4171\n",
            "Epoch 26/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.7432 - loss: 0.5361 - val_accuracy: 0.5512 - val_loss: 1.4858\n",
            "Epoch 27/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7498 - loss: 0.5178 - val_accuracy: 0.5562 - val_loss: 1.4033\n",
            "Epoch 28/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.7587 - loss: 0.4849 - val_accuracy: 0.5475 - val_loss: 1.4934\n",
            "Epoch 29/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.7603 - loss: 0.4657 - val_accuracy: 0.5663 - val_loss: 1.3997\n",
            "Epoch 30/30\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7559 - loss: 0.4696 - val_accuracy: 0.5706 - val_loss: 1.4158\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "Model Evaluation on Test Set:\n",
            "Accuracy: 0.7030\n",
            "Precision: 0.6465\n",
            "Recall: 0.7008\n",
            "F1 Score: 0.6680\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.92      0.86       134\n",
            "           1       0.84      0.87      0.85       194\n",
            "           2       0.69      0.77      0.73       162\n",
            "           3       0.58      0.68      0.63       150\n",
            "           4       0.71      0.90      0.80       153\n",
            "           5       0.75      0.69      0.72        39\n",
            "           6       0.83      0.84      0.83       291\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.73      0.81      0.77       211\n",
            "           9       0.83      0.83      0.83        47\n",
            "          10       0.48      0.50      0.49        22\n",
            "          11       0.57      0.69      0.63       134\n",
            "          12       0.56      0.69      0.62        32\n",
            "          13       0.77      0.88      0.82       201\n",
            "          14       0.78      0.80      0.79       210\n",
            "          15       0.61      0.78      0.68        51\n",
            "          16       0.69      0.75      0.72       108\n",
            "          17       0.59      0.70      0.64       123\n",
            "          18       0.47      0.21      0.29       454\n",
            "\n",
            "    accuracy                           0.70      2717\n",
            "   macro avg       0.65      0.70      0.67      2717\n",
            "weighted avg       0.68      0.70      0.68      2717\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This refined model outputs best results,\n",
        "so use this model for taking inputs and producuing predictions."
      ],
      "metadata": {
        "id": "KZS0tBfVJgKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "model.save('bilstm_attention_model.keras')\n"
      ],
      "metadata": {
        "id": "AkorYcleIg77"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example for testing the model performance using a random input\n",
        "import numpy as np\n",
        "\n",
        "def extract_relation(sentence, tokenizer, max_len, bilstm_attention_model, svm_model, label_encoder):\n",
        "    # Tokenize and pad the input sentence\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Generate features using the BiLSTM-Attention model\n",
        "    features = bilstm_attention_model.predict(padded_sequence)\n",
        "\n",
        "    # Predict the relation using the SVM classifier\n",
        "    predicted_relation_encoded = svm_model.predict(features)\n",
        "\n",
        "    # Decode the predicted relation\n",
        "    predicted_relation = label_encoder.inverse_transform(predicted_relation_encoded)\n",
        "\n",
        "    return predicted_relation[0]\n",
        "\n",
        "# Example usage:\n",
        "input_sentence = \"The company acquired a new startup in Silicon Valley.\"\n",
        "relation = extract_relation(input_sentence, tokenizer, max_len, model, svm_model, label_encoder)\n",
        "print(f\"Extracted Relation: {relation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQQQTfoJJPAK",
        "outputId": "23d73c44-d9c4-4f7a-8bc8-83e3238b41ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Extracted Relation: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "#  Step 1: Load the trained model\n",
        "model_path = \"/content/bilstm_attention_model.keras\"\n",
        "print(\"Loading model...\")\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Step 2: Load the tokenizer\n",
        "tokenizer_path = \"/content/tokenizer.pkl\"\n",
        "with open(tokenizer_path, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#  Step 3: Manually define relation ID mappings for SemEval 2010 Task 8\n",
        "relation_id_to_label = {\n",
        "          0: \"Cause-Effect(e1,e2)\",\n",
        "          1: \"Cause-Effect(e2,e1)\",\n",
        "          2: \"Component-Whole(e1,e2)\",\n",
        "          3: \"Component-Whole(e2,e1)\",\n",
        "          4: \"Content-Container(e1,e2)\",\n",
        "          5: \"Content-Container(e2,e1)\",\n",
        "          6: \"Entity-Destination(e1,e2)\",\n",
        "          7: \"Entity-Destination(e2,e1)\",\n",
        "          8: \"Entity-Origin(e1,e2)\",\n",
        "          9: \"Entity-Origin(e2,e1)\",\n",
        "          10: \"Instrument-Agency(e1,e2)\",\n",
        "          11: \"Instrument-Agency(e2,e1)\",\n",
        "          12: \"Member-Collection(e1,e2)\",\n",
        "          13: \"Member-Collection(e2,e1)\",\n",
        "          14: \"Message-Topic(e1,e2)\",\n",
        "          15: \"Message-Topic(e2,e1)\",\n",
        "          16: \"Product-Producer(e1,e2)\",\n",
        "          17: \"Product-Producer(e2,e1)\",\n",
        "          18: \"Other\"\n",
        "\n",
        "}\n",
        "\n",
        "#  Step 4: Define function for relation extraction\n",
        "def extract_relation_keras(sentence, model, tokenizer, relation_id_to_label, max_len=100):\n",
        "    \"\"\"\n",
        "    Extracts the relation using the trained Keras model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "        model (tf.keras.Model): The trained Keras model.\n",
        "        tokenizer (Tokenizer): The tokenizer used for training.\n",
        "        relation_id_to_label (dict): Dictionary mapping relation IDs to relation names.\n",
        "        max_len (int): Maximum sequence length.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (relation_id, relation_name)\n",
        "    \"\"\"\n",
        "    # Tokenize and pad the sentence\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Predict relation\n",
        "    features = model.predict(padded_sequence)\n",
        "    predicted_relation_id = np.argmax(features, axis=1)[0]  # Get predicted class index\n",
        "\n",
        "    # Retrieve the relation name\n",
        "    predicted_relation_name = relation_id_to_label.get(predicted_relation_id, \"Unknown Relation\")\n",
        "\n",
        "    return predicted_relation_id, predicted_relation_name\n",
        "\n",
        "#  Step 5: Real-time user input loop\n",
        "print(\"\\n Relation Extraction Model Ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    user_sentence = input(\"\\nEnter a sentence:\\n\")\n",
        "\n",
        "    if user_sentence.lower() == \"exit\":\n",
        "        print(\"Exiting program...\")\n",
        "        break\n",
        "\n",
        "    relation_id, relation_name = extract_relation_keras(user_sentence, model, tokenizer, relation_id_to_label)\n",
        "\n",
        "    print(f\" Predicted Relation ID: {relation_id}, Name: {relation_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "ay_cQtCwKCnz",
        "outputId": "15d2e467-0d32-4620-93f7-c6d7b82dca96"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "\n",
            " Relation Extraction Model Ready! Type 'exit' to stop.\n",
            "\n",
            "Enter a sentence:\n",
            "The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
            " Predicted Relation ID: 18, Name: Other\n",
            "\n",
            "Enter a sentence:\n",
            "The <e1>company</e1> fabricates plastic <e2>chairs</e2>.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            " Predicted Relation ID: 17, Name: Product-Producer(e2,e1)\n",
            "\n",
            "Enter a sentence:\n",
            "The school <e1>master</e1> teaches the lesson with a <e2>stick</e2>.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            " Predicted Relation ID: 11, Name: Instrument-Agency(e2,e1)\n",
            "\n",
            "Enter a sentence:\n",
            "The suspect dumped the dead <e1>body</e1> into a local <e2>reservoir</e2>.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            " Predicted Relation ID: 6, Name: Entity-Destination(e1,e2)\n",
            "\n",
            "Enter a sentence:\n",
            "Avian <e1>influenza</e1> is an infectious disease of birds caused by type A strains of the influenza <e2>virus</e2>.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted Relation ID: 1, Name: Cause-Effect(e2,e1)\n",
            "\n",
            "Enter a sentence:\n",
            "The <e1>ear</e1> of the African <e2>elephant</e2> is significantly larger--measuring 183 cm by 114 cm in the bush elephant.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            " Predicted Relation ID: 2, Name: Component-Whole(e1,e2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9888e55cf3f6>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0muser_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter a sentence:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.poor performance in relation no 7, try eliminating it and see the model performance**"
      ],
      "metadata": {
        "id": "IRJlsGyl6weV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_classes = [7]\n",
        "y_test_filtered = np.delete(y_test_encoded, np.where(y_test_encoded == excluded_classes[0])[0])\n",
        "y_pred_filtered = np.delete(y_pred, np.where(y_test_encoded == excluded_classes[0])[0])\n",
        "\n",
        "accuracy_filtered = accuracy_score(y_test_filtered, y_pred_filtered)\n",
        "precision_filtered = precision_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "recall_filtered = recall_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "f1_filtered = f1_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "\n",
        "print(\"Model Evaluation on Test Set (excluding class 7):\")\n",
        "print(f\"Accuracy: {accuracy_filtered:.4f}\")\n",
        "print(f\"Precision: {precision_filtered:.4f}\")\n",
        "print(f\"Recall: {recall_filtered:.4f}\")\n",
        "excluded_classes = [7]\n",
        "y_test_filtered = np.delete(y_test_encoded, np.where(y_test_encoded == excluded_classes[0])[0])\n",
        "y_pred_filtered = np.delete(y_pred, np.where(y_test_encoded == excluded_classes[0])[0])\n",
        "\n",
        "accuracy_filtered = accuracy_score(y_test_filtered, y_pred_filtered)\n",
        "precision_filtered = precision_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "recall_filtered = recall_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "f1_filtered = f1_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "\n",
        "print(\"Model Evaluation on Test Set (excluding class 7):\")\n",
        "print(f\"Accuracy: {accuracy_filtered:.4f}\")\n",
        "print(f\"Precision: {precision_filtered:.4f}\")\n",
        "print(f\"Recall: {recall_filtered:.4f}\")\n",
        "print(f\"F1 Score: {f1_filtered:.4f}\")\n",
        "print(\"\\nDetailed Classification Report (excluding class 7):\")\n",
        "print(classification_report(y_test_filtered, y_pred_filtered, target_names=[str(label_encoder.classes_[i]) for i in range(len(label_encoder.classes_)) if i not in excluded_classes]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UpxlhYcxDc5",
        "outputId": "cf8270ac-3070-4200-b20a-11e69116cda4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation on Test Set (excluding class 7):\n",
            "Accuracy: 0.6852\n",
            "Precision: 0.6568\n",
            "Recall: 0.7190\n",
            "Model Evaluation on Test Set (excluding class 7):\n",
            "Accuracy: 0.6852\n",
            "Precision: 0.6568\n",
            "Recall: 0.7190\n",
            "F1 Score: 0.6806\n",
            "\n",
            "Detailed Classification Report (excluding class 7):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       134\n",
            "           1       0.82      0.84      0.83       194\n",
            "           2       0.73      0.77      0.75       162\n",
            "           3       0.57      0.63      0.60       150\n",
            "           4       0.73      0.85      0.79       153\n",
            "           5       0.73      0.69      0.71        39\n",
            "           6       0.82      0.82      0.82       291\n",
            "           8       0.72      0.80      0.76       211\n",
            "           9       0.84      0.81      0.83        47\n",
            "          10       0.32      0.50      0.39        22\n",
            "          11       0.56      0.64      0.60       134\n",
            "          12       0.37      0.62      0.47        32\n",
            "          13       0.76      0.88      0.82       201\n",
            "          14       0.73      0.83      0.78       210\n",
            "          15       0.66      0.78      0.71        51\n",
            "          16       0.69      0.75      0.72       108\n",
            "          17       0.55      0.67      0.60       123\n",
            "          18       0.44      0.20      0.28       454\n",
            "\n",
            "    accuracy                           0.69      2716\n",
            "   macro avg       0.66      0.72      0.68      2716\n",
            "weighted avg       0.67      0.69      0.67      2716\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The improvement from eliminating relation class 7 is little."
      ],
      "metadata": {
        "id": "M-VNpzIf63TN"
      }
    }
  ]
}
